{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGQU8os7GrnDx3A0/Y6EHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille055/AIPI531/blob/main/Homework1_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AIPI531: Deep Reinforcement Learning** ##\n",
        "\n",
        "#Homework 1, Q3 Due date 2/7/2023#\n",
        "\n",
        "NAME = Chad Miller\n",
        "\n"
      ],
      "metadata": {
        "id": "FUHHtNcCH2xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the n-step advantage with n-step return (mentioned in the class), vanilla advantage, GAE, as well as MC advantage for A2C algorithm:\n",
        "Hint: SB3 implements Generalized Advantage Estimation (GAE) for A2C. In particular, you can find the implementation of the advantage in the method def compute_returns_and_advantage method in buffer.py (stable-baselines3/stable_baselines3/common/buffers.py) (https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/buffers.py). You can also play with the hyper-parameter (gae_lambda) to get different advantages. [Requirements]:\n",
        "- Compare the n-step advantage with the (vanilla) advantage, MC advantage, as well as GAE. Note that MC advantage is just optional for this assignment.\n",
        "[Software Setup for Development]:\n",
        "1. gitclone the SB3 source code repo(developmentversion)to your local machine (no need to install it on your local machine). https://stable-baselines3.readthedocs.io/en/master/guide/install.html\n",
        "2. Download the Google Drive Desktop(to sync with your Google Drive).\n",
        "https://www.google.com/drive/download/\n",
        "     \n",
        "3. Sync the cloned SB3 code repo(on your local machine) with the one in your Google Drive using Google Drive Desktop.\n",
        "4. Connect your cloned SB3 code repo stored in your Google Drive to Google Colab.\n",
        "5. Install the cloned SB3 code repo in your Google Drive using Google Colab (install it on Colab).\n",
        "6. Use any editor (e.g.Sublime,PyCharm,etc.) for the code implementation (modify the code in the cloned SB3 on your local machine).\n",
        "7. Test your implementation and running experiments using Colab.\n",
        "8. Push your code in the cloned SB3 on your local machine to your repo in your GitHub account.\n",
        "[Submission]:"
      ],
      "metadata": {
        "id": "4LKMOK_lIECg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install 'git+https://github.com/mille055/AIPI531/stable-baselines3.git'"
      ],
      "metadata": {
        "id": "CjI5yoL5tJbI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9jUVZkD9HJO",
        "outputId": "dabec177-52e4-4950-ee61-5be31a6c3f38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install '/content/drive/Othercomputers/My MacBook Air/AIPI531/stable-baselines3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Ow5uJ6800Z",
        "outputId": "0e5007a2-4a4d-4f84-e5d3-4b4f3bdd434d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./drive/Othercomputers/My MacBook Air/AIPI531/stable-baselines3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable-baselines3==1.8.0a3) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3==1.8.0a3) (1.13.1+cu116)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable-baselines3==1.8.0a3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3==1.8.0a3) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable-baselines3==1.8.0a3) (3.2.2)\n",
            "Collecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3==1.8.0a3) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable-baselines3==1.8.0a3) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3==1.8.0a3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3==1.8.0a3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3==1.8.0a3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3==1.8.0a3) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3==1.8.0a3) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3==1.8.0a3) (1.15.0)\n",
            "Building wheels for collected packages: stable-baselines3, gym\n",
            "  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-baselines3: filename=stable_baselines3-1.8.0a3-py3-none-any.whl size=172079 sha256=ac86791aac1f603d5ad4c7d924f59f17cd0a796fcc488ae0a291ec8226d584fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/e3/4d/0ee421d4b7118aa9b636e4a5f0999b8e781d54caa70be63107\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=e95d8f95622c8c8b64372006aa8147c299b6f6608fbfb3a5b5640eee7c89f08d\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "Successfully built stable-baselines3 gym\n",
            "Installing collected packages: importlib-metadata, gym, stable-baselines3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.21.0 importlib-metadata-4.13.0 stable-baselines3-1.8.0a3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library imports\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "Sez6bcwKZCdA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import NamedTuple\n",
        "name = 'CartPole-v1'\n",
        "\n",
        "q3_env = gym.make(name)\n"
      ],
      "metadata": {
        "id": "cDqQNmveZ9Uq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_reward(statement, env, gae_lambda, steps):\n",
        "  model = A2C('MlpPolicy', env, gae_lambda=gae_lambda)\n",
        "  model.learn(total_timesteps=steps)\n",
        "  mean_reward, std_reward = evaluate_policy(model, env)\n",
        "  print(\"Mean reward for {} with gae_lambda {} is {}.\".format(statement,gae_lambda,mean_reward))"
      ],
      "metadata": {
        "id": "8ZrSeCX2AHMQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 100000\n",
        "\n",
        "calculate_mean_reward(\"using GAE advantage\", q3_env, 0.9, steps)\n",
        "calculate_mean_reward(\"using n-step advantage\",q3_env, 1, steps)\n",
        "calculate_mean_reward(\"using vanilla advantage\",q3_env, 0, steps)"
      ],
      "metadata": {
        "id": "peCzI5GrALnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest mean reward when using GAE advantage. Lowest when using vanilla advantage."
      ],
      "metadata": {
        "id": "_Y1iNBI7Dbjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another approach -- adding parameter 'n_step_advantage' to the method definition of compute_return_and_advantage method in buffer.py\n"
      ],
      "metadata": {
        "id": "y64-rKfeuP3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train with n-step advantage (set n_step_advantage = True and default gae_lambda value)"
      ],
      "metadata": {
        "id": "7K8C2P6-v3YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_n_step_adv = A2C('MlpPolicy', q3_env, verbose=1, tensorboard_log=\"./a2c_\"+name+\"tensorboard/\", n_step_advantage=True)\n",
        "model_n_step_adv.learn(total_timesteps = steps)\n",
        "\n",
        "## Evaluating the agent using 100 episodes\n",
        "reward_mean, reward_std = evaluate_policy(model_n_step_adv, q3_env, n_eval_episodes=100)\n",
        "print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')\n"
      ],
      "metadata": {
        "id": "sCBuuR6gwCCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train with vanilla (n_step_advantage = False, gae_lambda = 0)"
      ],
      "metadata": {
        "id": "I61tLulPw5OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vanilla = A2C('MlpPolicy', q3_env, verbose=1, gae_lambda = 0, tensorboard_log=\"./a2c_\"+name+\"tensorboard/\", n_step_advantage=False)\n",
        "model_vanilla.learn(total_timesteps = steps)\n",
        "\n",
        "## Evaluating the agent using 100 episodes\n",
        "reward_mean, reward_std = evaluate_policy(model_vanilla, q3_env, n_eval_episodes=100)\n",
        "print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')\n"
      ],
      "metadata": {
        "id": "4Ju3xvM0xCnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train with vanilla (n_step_advantage = False, gae_lambda = 0)"
      ],
      "metadata": {
        "id": "UPejqpqdx1AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gae = A2C('MlpPolicy', q3_env, verbose=1, tensorboard_log=\"./a2c_\"+name+\"tensorboard/\", n_step_advantage=False)\n",
        "model_gae.learn(total_timesteps = steps)\n",
        "\n",
        "## Evaluating the agent using 100 episodes\n",
        "reward_mean, reward_std = evaluate_policy(model_gae, q3_env, n_eval_episodes=100)\n",
        "print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"
      ],
      "metadata": {
        "id": "r9R1TaFTxavl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_M1J_xHR5ttn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}