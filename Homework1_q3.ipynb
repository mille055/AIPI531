{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVZRL5cwe/ZQtll5w6Y/4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille055/AIPI531/blob/main/Homework1_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AIPI531: Deep Reinforcement Learning** ##\n",
        "\n",
        "#Homework 1, Due date 2/7/2023#\n",
        "\n",
        "NAME = Chad Miller\n",
        "\n"
      ],
      "metadata": {
        "id": "FUHHtNcCH2xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the n-step advantage with n-step return (mentioned in the class), vanilla advantage, GAE, as well as MC advantage for A2C algorithm:\n",
        "Hint: SB3 implements Generalized Advantage Estimation (GAE) for A2C. In particular, you can find the implementation of the advantage in the method def compute_returns_and_advantage method in buffer.py (stable-baselines3/stable_baselines3/common/buffers.py) (https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/buffers.py). You can also play with the hyper-parameter (gae_lambda) to get different advantages. [Requirements]:\n",
        "- Compare the n-step advantage with the (vanilla) advantage, MC advantage, as well as GAE. Note that MC advantage is just optional for this assignment.\n",
        "[Software Setup for Development]:\n",
        "1. gitclone the SB3 source code repo(developmentversion)to your local machine (no need to install it on your local machine). https://stable-baselines3.readthedocs.io/en/master/guide/install.html\n",
        "2. Download the Google Drive Desktop(to sync with your Google Drive).\n",
        "https://www.google.com/drive/download/\n",
        "     \n",
        "3. Sync the cloned SB3 code repo(on your local machine) with the one in your Google Drive using Google Drive Desktop.\n",
        "4. Connect your cloned SB3 code repo stored in your Google Drive to Google Colab.\n",
        "5. Install the cloned SB3 code repo in your Google Drive using Google Colab (install it on Colab).\n",
        "6. Use any editor (e.g.Sublime,PyCharm,etc.) for the code implementation (modify the code in the cloned SB3 on your local machine).\n",
        "7. Test your implementation and running experiments using Colab.\n",
        "8. Push your code in the cloned SB3 on your local machine to your repo in your GitHub account.\n",
        "[Submission]:"
      ],
      "metadata": {
        "id": "4LKMOK_lIECg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "oxtdHRGPHEUD",
        "outputId": "18d387c5-2d9b-4997-a949-013575b91b32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-38142f704f6e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    AIPI531: Deep Reinforcement Learning\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#git install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library imports\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "Sez6bcwKZCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'CartPole-v1'\n",
        "\n",
        "# Training environment\n",
        "train_env = gym.make(name)\n",
        "\n",
        "# Evalutation environment \n",
        "evaluation_env = gym.make(name)"
      ],
      "metadata": {
        "id": "cDqQNmveZ9Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train using 'n-step-advantage'\n"
      ],
      "metadata": {
        "id": "ovi5jgwdad28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an Agent (Actor Critic)\n",
        "model_n_step = A2C('MlpPolicy', train_env, verbose=1, tensorboard_log=\"./a2c\"+name+\"_tensorboard/\", use_n_step_advantage=True)\n",
        "\n",
        "## Training the agent for 100000 steps\n",
        "model_n_step.learn(total_timesteps=100000)"
      ],
      "metadata": {
        "id": "PEmWVojoaLUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluating the agent using 100 episodes\n",
        "reward_mean, reward_std = evaluate_policy(model_n_step, evaluation_env, n_eval_episodes=100)\n",
        "print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"
      ],
      "metadata": {
        "id": "7-Um4jW4a3Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train using 'no GAE'"
      ],
      "metadata": {
        "id": "yiwkN-GyHy-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize an Agent (Actor Critic)\n",
        "model_no_gae = A2C('MlpPolicy', train_env, verbose=1, gae_lambda=0, tensorboard_log=\"./a2c\"+name+\"_tensorboard/\", use_n_step_advantage=False)\n",
        "\n",
        "## Training the agent for 100000 steps\n",
        "model_no_gae.learn(total_timesteps=100000)"
      ],
      "metadata": {
        "id": "v7ISd_RYbIod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}